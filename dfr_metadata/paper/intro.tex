
\section{Introduction}

Both in corporate and government settings, digital forensic (DF) tools are used for post-mortem investigation of cyber-crimes and cyber-attacks. 
Nowadays it is common \cite{df:news} for the police to use DF tools to follow an eletronic trail of evidence to track down the suspect. 
To maintain the quality and integrity of DF tools, National Institute of Standards and Technology (NIST)'s 
Computer Forensics Tool Testing Program (CFTT) \cite{cftt:nist} 
set standards for these tools. Maintaining the standards of DF tools 
is especially critical for judicial proceedings: Usage of a forensic tool that does not follow the standards may cause evidence to be thrown 
out in a court case whereas incorrect results from a forensic tool can also lead improper prosecution of an innocent defendant. 

The focus of this paper is about standardization of one class of DF 
tools that are for Deleted File Recovery (DFR) \cite{meta:dfr:standards}. 
As the name suggests, a DFR tool attempts to retrieve deleted files
from a file system of a computer. As an example, given a hard disk or a USB drive 
(which might have been seized from the suspect computer or collected from the crime scene), a 
forensics professional can use a DFR tool to investigate about (and potentially retrieve) deleted files that 
a suspect may have deleted to hide important information. 
The success or failure of a DFR tool can decide the outcome of a case.  

Our experiments with a popular DF tool suite (named Autopsy \cite{autopsy}) 
show that it does not satisfy all NIST standards for DFR. 
Furthermore, we extensively experimented with other frequently used DFR tools. 
We compare those tools' performance and compile a comparative analysis, which could help the user choose the right DFR tool. 

Evaluating the performance of a DFR tool is complex because many elements of a forensics scenario determine 
the success or failure of the file recovery process. 
A few such factors are the type of the file system (FAT, NTFS, etc.), presence of other active/deleted 
files in the file system, fragmentation of a file, a deleted file being overwritten by another file, and so on.
So, comparison of two DFR tools is scientific only if they are compared while keeping these factors same. 
Via extensive analysis, we design a set of test file system images (for either of FAT and NTFS) which considers each of the above factors independently. 
We claim that this list of test cases is exhaustive and thus claim that our evaluation gives a complete picture. 
 

The main contributions of the paper are listed as follows:
\begin{itemize}
\item We design and build an exhaustive list of canonical test file system (FAT and NTFS) images to test the DFR tool per NIST standards. 
\item We perform evaluation of frequently-used DFR tools (including free tools as well as proprietary ones) on the test images.
\item For the interesting cases of tools' success or failure, we provide logical explanation.
\item We provide critique on applicability of some of the NIST standards in a practical setting. 
\end{itemize}


The NIST CFTT portal currently publishes reports of only a subset of DFR tools. 
However, the scope of tools needs to be expanded as many new tools come to market and become popular.
Also, existing DFR tools should be retested to ensure their reliability is consistent 
as new patches and features come out. 
Adding new reports to the CFTT website will allow tool developers a 
chance to continually develop their tool for the better. We will submit our study reports to the CFTT portal.

As a side benefit, our work leads to a few hands-on lab-modules to be used in digital forensics courses 
at BGSU, enriching the new DF specialization program in the CS department. We will also make these modules
available for relevant instructors at other universities.
