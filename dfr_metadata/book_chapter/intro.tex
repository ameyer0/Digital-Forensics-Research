\section{Introduction}\label{intro}


Nowadays goverment organizations as well as private enterprises encounter cyber-attacks quite frequently.
After such an attack law enforcement typically conducts a digital forensics (DF) investigation~\cite{df:news} \morecite 
with the goal of identifying what caused the attack, how it was executed, and who are the potential perpetrators. 
In addition to cyber-attacks, we encounter many other digital crimes, such as steal of intellectual properties, 
compromise of private data, and more. To conduct post-mortem analysis of cyber-attacks and digital crimes, forensics professionals
employ multiple DF tools. 

The success of the investigation depends on the accuracy and overall quality of DF tools.
Furthermore, as the forensics investigation often culminates with a judicial proceeding in the court of law, integrity of DF tools 
is critical: On one hand, a court may throw out a piece of evidence if it was collected by a DF tool that does not follow the standards, 
and on the other hand, inaccurate results from a DF tool can cause improper prosecution of an innocent defendant. 
DF tools that are available in the market come from many vendors, and the government needs to keep an eye on the integrity of these tools. 
As a standardization initiative of DF tools,  Computer Forensics Tool Testing Program (CFTT)~\cite{cftt:nist} 
at National Institute of Standards and Technology (NIST) published a list of expectations for these tools. 

In this article, we consider standardization of Deleted File Recovery (DFR)~\cite{meta:dfr:standards} \morecite tools which is a special type of DF tools. 
Given a storage device such as hard disk or a USB drive, we use a DFR tool to search for and retrieve deleted files.
DFR tools are widely used in real-life DF investigations. For instance, after capturing a hard disk from a suspect's compuuter a
DF professional uses a DFR tool to retreive the files that the suspect might have deleted to destroy some important artifacts.
A retrieved file might add critical evidence to a case. In other words, success or failure of a DFR tool can sway the outcome of a case.

Depending on the working principle, we can classify DFR tools into two subtypes: \em{Metadata-based DFR} tools and \em{file carving} tools.
The first subtype utilizes the file system metadata (if available) to identify (a.k.a. recover) a deleted file. 
The second subtype (i.e., a file carving tool) does not rely on the file system metadata and instead utilizes target file's 
header and footer signature. In this article, we independently study both \em{Metadata-based DFR} tools and \em{file carving} tools.      

\begin{paraphrase}

Our experiments with a popular DF tool suite named Autopsy~\cite{autopsy} 
show that it does not meet all NIST expectations for DFR. 
Furthermore, we extensively experimented with other frequently used DFR tools. 
We compare those tools' performance and compile a comparative analysis, which could help the user choose the right DFR tool. 

Evaluating the performance of a DFR tool is complex because many elements of a forensics scenario determine 
the success or failure of the file recovery process. 
A few such factors are the type of the file system (FAT, NTFS, etc.), presence of other active/deleted 
files in the file system, fragmentation of a file, a deleted file being overwritten by another file, and so on.
So, comparison of two DFR tools is scientific only if they are compared while keeping these factors same. 
Via extensive analysis, we design a set of test file system images (for either of FAT and NTFS) which considers each of the above factors independently. 
We claim that this list of test cases cover most of the scenarios (except few edge cases) in real-life, and thus claim that our evaluation has broad coverage. 

As there are many file systems (e.g., ext4, HFS, etc.) in addition to FAT and NTFS, one might be interested to know why we chose FAT and NTFS for the current work. 
Because FAT and NTFS are very widely used on external storage devices and devices running Microsoft Windows, respectively,
real-life forensics investigation often involves these two file systems.
While we leave other file systems for future study, our current methodology could be 
extended to other file systems to make a similar study.

The main contributions of the paper are listed as follows:
\begin{itemize}
\item We design and build a list of canonical test file system (FAT and NTFS) images to evaluate the DFR tool per NIST guidelines. 
\item We perform evaluation of frequently-used DFR tools (including free tools as well as proprietary ones)\footnote{As the subject of our evaluation study, 
we have chosen a few tools or trade names. In no case does such choosing imply recommendation or endorsement by the authors.} on the test images.
\item For the interesting cases of tools' success or failure, we provide logical explanation.
\item We provide critique on applicability of some of the NIST guidelines in a practical setting. 
\end{itemize}


The NIST CFTT portal currently publishes reports of only a subset of DFR tools. 
However, that set needs to be expanded as many new tools come to market and become popular.
Also, existing DFR tools should be retested to ensure their reliability is consistent 
as new patches and features come out. 
Adding new reports to the CFTT website will allow tool developers a 
chance to continually develop their tools for the better. We will submit our study reports to the CFTT portal.
At the time of writing, the portal publishes reports for Autopsy~\cite{dhs:autopsy} and FTK~\cite{dhs:ftk}; 
however, both reports are from 2014 and evaluate now-outdated versions of the software.

As a side benefit, our work leads to a few hands-on lab-modules to be used in digital forensics courses 
at BGSU, enriching the new DF specialization program in the CS department. We will also make these modules
publicly available to be used by relevant instructors at other universities.

\end{paraphrase} 
