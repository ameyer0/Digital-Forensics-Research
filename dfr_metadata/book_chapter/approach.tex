\section{Approach}
\subsection{Overview}

To properly evaluate DFR tools, we must see how they perform in various file recovery scenarios.
We do this by running the tools on various \emph{disk images}, each containing a file system with some deleted files.
The recovered files output by a DFR tool are examined to see how well the tool meets the NIST guidelines for deleted file recovery.
By using test images designed to each emphasize specific recovery tasks, we can get a sense of a tools' strengths and weaknesses, and what tasks are especially difficult for certain types of tools.
With this in mind, we designed several disk images to test metadata-based DFR tools.
NIST CFTT has already designed a set of images for testing file carving tools, so we use those rather than creating our own.
A high-level illustration of our methodology can be seen in Figure~\ref{fig:overview}.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/overview.pdf}
    \caption{
        A read-only \emph{disk image}, a file which contains the raw data of a file system, is obtained for each test case.
        For metatada-based test cases, we create a file system on an external drive and delete files from it, then save it as a disk image.
        For file carving test cases, we use a set of disk images prepared by NIST CFTT.
        In either case disk image is input to a DFR tool, whose output is checked for compliance with the NIST guidelines for metadata-based DFR or file carving, depending on the tool.
    }
    \label{fig:overview}
\end{figure}

\subsection{Metadata-Based Tools}
\subsubsection{Designing Recovery Scenarios}

We begin by designing a set of test cases to simulate common challenges of metadata-based deleted file recovery.
In order to get the most information possible about each tools' capabilities, we design our test cases to be atomic.
We first isolate the most basic challenges of file recovery, and create test cases for them.
Then, we create test cases which combine those basic challenges.
Our intention with this atomic approach is to create test cases that are generalizeable to the majority of recovery scenarios, even the many which we do not explicitly cover.

In addition to this philosophy, we must remain within the scope of the NIST guidelines.
NIST requires test images to be ``created and deleted in a process similar to how an end-user would create and delete files.''~\cite{meta:dfr:standards}
We take this to mean that any interaction with the file system must be through a standard operating systems'  read and write operations.
We are not allowed to edit the file system directly, as ``files and file system metadata that is specifically corrupted, modified, or otherwise manipulated to appear deleted''~\cite{meta:dfr:standards} are explicitly out of scope.

Within these constraints, we can induce two phenomena which make file recovery more challenging: fragmentation and overwriting.
Following the afforementioned goal of making our tests atomic, all our test cases (besides the first trivial case) use fragmented and/or overwritten files as building blocks.
As such, our test cases fall into five general categories:
{\bf(a)} no fragmentation or overwriting, 
{\bf (b)} only fragmentation,
{\bf (c)} only overwriting,
{\bf (d)} both fragmentation and overwriting,
and {\bf (e)} fragmentation ``out of order.''


Following is a description of each test case. A selection of test cases are illustrated, with each column portraying the state of the file system at a point in time. Each file is given a unique letter and shading, and the start and end of a fragmented file are denoted when relevant.

\paragraph{Basic Deleted File}
\begin{arabiclist}
    \item Deleted file is contiguous and the only file on the disk
\end{arabiclist}
\paragraph{Deleted File is Fragmented}
\begin{arabiclist}[2]
\setcounter{enumi}{1}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{fig/case2.pdf}
        \caption{Disk 2. File A is fragmented, and has been deleted.}
        \label{fig:case2}
    \end{figure}
    \item Deleted file is fragmented around an active file (as illustrated in Figure~\ref{fig:case2})
    \item Deleted file is fragmented around another deleted file
\end{arabiclist}
\paragraph{Deleted File is Overwritten}
\begin{arabiclist}
\setcounter{enumi}{3}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{fig/case5.pdf}
        \caption{Disk 5. File A has been deleted and partially overwritten by file B.}
        \label{fig:case5}
    \end{figure}
    \begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{fig/case8.pdf}
        \caption{Disk 8. File A has been deleted and partially overwritten by file B, which has since been deleted}
        \label{fig:case8}
    \end{figure}
    \item Deleted file is overwritten at the front by an active file
    \item Deleted file is overwritten in the middle by an active file (as illustrated in Figure~\ref{fig:case5})
    \item Deleted file is overwritten by an active file that does not end on a sector boundary (size is not a multiple of 512 bytes)
    \item Deleted file is completely overwritten by an active file
    \item Deleted file is overwritten at the front by another deleted file (as illustrated in Figure~\ref{fig:case8})
    \item Deleted file is overwritten in the middle by another deleted file
    \item Deleted file is overwritten by another deleted file that does not end on a sector boundary (size is not a multiple of 512 bytes)
    \item Deleted file completely overwritten by another deleted file
\end{arabiclist}
\paragraph{Deleted File is Fragmented and Overwritten}
 \begin{arabiclist}
 \setcounter{enumi}{11}
     \begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{fig/case12.pdf}
        \caption{Disk 12. File A is fragmented and has been deleted. The second fragment has then been overwritten by file C}
        \label{fig:case12}
    \end{figure}
    \item Deleted file is fragmented around an active file, and the second fragment is overwritten by another active file (as illustrated in Figure~\ref{fig:case12})
    \item Deleted file is fragmented around an active file, and the second fragment is overwritten by another deleted file
 \end{arabiclist}

\paragraph{Deleted File is Fragmented Out-of-Order}
\begin{arabiclist}
\setcounter{enumi}{13}
    \begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{fig/case14.pdf}
        \caption{Disk 14. File A is fragmented out-of-order, and has been deleted}
        \label{fig:case14}
    \end{figure}
    \item Deleted file fragmented out-of-order (as illustrated in Figure~\ref{fig:case14})
    \item Deleted file fragmented out-of-order with an active file in between the fragments
    \item eleted file fragmented out-of-order with a deleted file in between the fragments
\end{arabiclist}



Fragmentation is trivial in NTFS because information about the runs of a file persist after deletion.
Since this renders Cases 2, 3, 14, 15, and 16 functionally identical, we only test Cases 2 and 3 for NTFS.
Cases 5 and 9 cannot be created through regular file operations in NTFS, due to NTFS's file allocation behavior.
Thus, we also exclude Cases 5 and 9 for NTFS.
We do not exclude any test cases when using FAT.


\subsubsection{Creating Test Images}

We create the test cases on a 32 GB flash drive, using 4 MiB partitions for FAT cases and 6 MiB partitions for NTFS cases.
For each case, it is important to start by writing over the partition with zeroes, ensuring the image is easily reproduceable.
Next, a file system should be written to the partition, and files should be written to it and deleted until the file system matches one of the planned test cases.
We used text files, each containing a single repeated letter (e.g., ``aa1M'' is a text file containing 1 MiB of the letter `a')
Note that text files cannot be recovered with file carving, so this choice forces tools which combine both DFR methods to rely solely on metadata-based recovery.
We write files to the test file system by copying them from another drive, and appending to files when we need to force fragmentation.
Once the test file system matches one of the planned test cases, we use the \emph{dd} utility to create a disk image of that partition.
Since the disk image can be made read-only, it is safer and more convenient to run tests on the disk image instead of the physical drive.
Note that we create FAT test cases using Ubuntu 18.04 and NTFS test cases using Windows 10.

\subsubsection{Challenges}

 % Caching problem
Creating test images can be difficult without a solid understanding of low-level file system behavior.
For example, a newly written file may not have its data written to the disk right away.
Even with fast modern storage, disk I/O involves a considerable amount of overhead.
To improve performance, operating systems generally prefer to write data in one larger batch versus several small batches.
So, the operating system will often store write operations in a cache and wait for a more optimal moment to actually write to the disk.
Typically, this optimization only causes problems in the event of sudden power loss or improper shutdown, but it can also make the state of the file system less predictable.
We found that we were often deleting a file while it was still cached, meaning it would never be written to the disk to begin with.
This is obviously a problem as it leaves nothing on the disk to recover.
Thankfully, Linux and Windows both provide a \emph{sync} system call, which causes the cached writes to be performed immediately.
Calling sync before each file deletion resolves the issue; alternatively, unmounting the file system triggers similar behavior.

% Learning and using the allocation algorithms
Since most of the test images involve manipulating file data into certain arrangments, it is also helpful to have some understanding of allocation algorithms.
Allocation algorithm refers to the steps the operating system takes to decide where on the disk to write new data.
Generally, the operating system wants to limit fragmentation, as contiguous files are more efficient to read and write to.
Common allocation algorithms like ``first available,'' ``next available,'' and ``best fit'' will place files according to that general principle, but use differing strategies to do so.
Understanding the allocation algorithm removes a lot of trial and error from the process of making test images.
For example, we observed that Linux uses a ``next available'' algorithm when writing to FAT.
After mounting the file system, the first file to be written will be placed at the first available space in the file system.
However, unlike the ``first available'' algorithm, the operating system remembers where it last placed a file, and it will place the next file at the first available space after that saved location, even if space opens up at the beginning of the file system.
Windows, on the other hand, uses a ``best fit'' algorithm when writing to NTFS.
This allocation algorithm takes a more proactive approach to reducing fragmentation by writing each file to the smallest space in which it can fit without being fragmented.


\subsubsection{Recovering Files}
\begin{paraphrase}
 We selected five popular DFR tools for testing: Autopsy~\cite{autopsy}, Recuva~\cite{recuva}, FTK Imager~\cite{ftk}, TestDisk~\cite{testdisk}, and Magnet AXIOM~\cite{axiom}. 
Note that Autopsy uses a set of DF tools known as The Sleuth Kit (TSK) for metadata-based recovery, so TSK is also implicitly covered by this study. 
These tools were chosen based on popularity and availability.

The settings we used when testing each tool are as follows:
For Autopsy, we performed a standard recovery with all ingest modules disabled.
For Recuva we performed a standard recovery using the free version with default settings.
For FTK Imager, we performed a standard recovery using the free version with default settings.
For TestDisk we used the ``file undelete'' feature under ``Advanced Filesystem Utils.''
For Magnet AXIOM we performed a ``full scan'' in AXIOM Process and exported all files accessible in ``Filesystem View'' in AXIOM Examine.
\end{paraphrase}

\subsubsection{Results}
\begin{paraphrase}

\begin{figure}
    \centering

    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/autopsy_results_fat.pdf}
        \subcaption{Autopsy}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/recuva_results_fat.pdf}
        \subcaption{Recuva}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/ftk_results_fat.pdf}
        \subcaption{FTK}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/testdisk_results_fat.pdf}
        \subcaption{TestDisk}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/axiom_results_fat.pdf}
        \subcaption{Magnet AXIOM}
    \end{subfigure}~~
        
    \caption{Test results on metadata-based DFR tools using FAT-formatted test images. Each row corresponds to a test image and each collumn corresponds to a core feature. An 'O' indicates success and an 'X' indicates failure.}
    \label{fig:results_fat}
\end{figure}

\begin{figure}
    \centering

    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/autopsy_results_ntfs.pdf}
        \subcaption{Autopsy}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/recuva_results_ntfs.pdf}
        \subcaption{Recuva}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/ftk_results_ntfs.pdf}
        \subcaption{FTK}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/testdisk_results_ntfs.pdf}
        \subcaption{TestDisk}
    \end{subfigure}~~
    \begin{subfigure}[t]{0.17\linewidth}
        \includegraphics[width=\linewidth]{fig/axiom_results_ntfs.pdf}
        \subcaption{Magnet AXIOM}
    \end{subfigure}~~
        
    \caption{Test results on metadata-based DFR tools using NTFS-formatted test images. Each row corresponds to a test image and each collumn corresponds to a core feature. An 'O' indicates success and an 'X' indicates failure.}
    \label{fig:results_ntfs}
\end{figure}

 After testing each tool, we analyzed the recovered object(s) from each test case. 
If the recovered file is identical to the original, obviously all expectations have been met. 
While this is ideal, it is often impossible to perfectly recover a file (such as when it is overwritten) so the standards do not require it. 

In our results, the file is only ever recovered perfectly in FAT cases 1 and 2, and NTFS cases 1-3. 
For all other cases, the tool is judged on each core feature individually. 
These judgments are summarized in Figure~\ref{fig:results_fat} for FAT test cases and Figure~\ref{fig:results_ntfs} for NTFS test cases.

For cases in which a tool does not fulfill core feature 1, in other words, it cannot find a deleted file, we make no judgment about the remaining core features.

\paragraph{Recovering Fragmented Files}
In cases of fragmentation in FAT file systems, we found each tool generally approaches recovery in one of two ways. 
Recuva and Magnet AXIOM start from the beginning of the file and recover the full length of the file even if an active file exists in that space. 
Autopsy, FTK, and TestDisk will start from the beginning of the file and recover the full length, but skip over any active files they encounter.
Autopsy, FTK, and TestDisk recover all of file A, while Recuva and Magnet AXIOM's recovered images erroneously contain data from file B, causing them to fail core feature 4. 
When the space in between fragments is unallocated, all tools recover the file as though it was contiguous, pulling some erroneous data and failing core feature 4. 
When the fragmentation occurs at the end of the file system, Recuva, FTK, and TestDisk recover only the first fragment, while Autopsy returns a short file of null data, and Magnet AXIOM reports an error and returns an empty file.
Cases with fragmentation are trivial for NTFS file systems as more information is available from the metadata. 
Unsurprisingly, no tools had problems with fragmentation cases for NTFS.

\paragraph{Recovering Overwritten Files}
In cases where a file has been overwritten by an active file, we found most tools recover the deleted file as though it is not overwritten, failing core feature 4. 
A few exceptions are FTK Imager, which recovers the file up to the point where it has been overwritten, and Autopsy, which generally recovers only the first cluster of an overwritten file in FAT, and behaves like the other tools for NTFS. 
TestDisk also exhibits the same behavior as FTK for FAT case 4ii only. 
Strangely, Magnet AXIOM's recovered objects for FAT cases 4i and 4ii include the overwritten sections, but nothing after them.
Other Magnet AXIOM results were similar to the other tools.
When the overwriting file has also been deleted, all tools recover the first file as though it is not overwritten.

\paragraph{Abnormal Results}
A few results stand out as unusual.
These are cases for which it is difficult to infer from the recovered object what approach a tool is using.

For FAT cases 4ii, 6, 8, 9, and 10, Autopsy returns a 1.5 KiB file of null data.
1.5 KiB is equivalent to 3 sectors, while a FAT cluster in our cases is defined as 4 sectors (2 KiB).

TestDisk fails to identify a file for NTFS cases 4iii and 4iv only. 
These are the only test cases in which a tool does not fulfill core feature 1.

For FAT cases 4i and 4ii, Magnet AXIOM does not recover the entire length of the deleted file, but it also does not exclude the overwritten sections. 
In both cases, it recovers up to the end of the overwritten sections, rather than up to the beginning like FTK does.
\end{paraphrase}




\subsection{Carving-Based Tools}

\subsubsection{CFTT Test Cases}
To evaluate the file carving tools, we used six disk images prepared by CFTT\cite{cftt_carving_images}.
Each of these images contains between 20 and 40 graphical files of the JPG, PNG, GIF, BMP, and TIF formats.
Importantly, the CFTT images do not contain valid file-systems.
This ensures that a tool which utilizes both DFR methods can be evaluated solely on its file carving ability.

Despite the differences between the methods, metadata-based DFR and file carving are complicated by the same two file-system behaviors: fragmentation and overwriting. Thus, the CFTT test cases are somewhat similar to the ones we created for metadata-based DFR; each is built around a specific variation of fragmentation or overwriting of deleted files.
Following are summaries of each CFTT test image:
\begin{itemize}
 \item \textbf{basic:} 40 contiguous files, with space in between each file
 \item \textbf{nofill:} 40 contiguous files, with no space in between each file
 \item \textbf{simple-frag:} 40 fragmented files, with space in between each fragment
 \item \textbf{braid:} 10 contiguous files and 10 fragmented files, which are fragmented around each other in an A-B-A-B pattern
 \item \textbf{disorder:} 35 fragmented files, 30 of which are fragmented out-of-order such that the fragment containing the footer comes before the fragment containing the header
 \item \textbf{partials:} 15 complete files, 5 of which are fragmented, and 25 incomplete files where at least one fragment has been overwritten or manually destroyed

\end{itemize}


\subsubsection{Recovering Files}

 We selected four popular file carving tools for testing: PhotoRec~\cite{photorec}, Foremost~\cite{foremost}, Scalpel~\cite{scalpel}, and Magnet AXIOM~\cite{axiom}.

The settings we used when testing each tool are as follows:
For PhotoRec and Foremost, we used the default settings.
For Scalpel, we manually enabled recovery of the JPG, PNG, GIF, BMP, and TIF formats, and otherwise used default settings.
For Magnet Axiom, we ran a ``full scan'' with default settings in AXIOM Process and exported all graphical files from ``Artifact View'' in AXIOM Examine.
We also exported an XML carving report from Axiom Examine as unlike the other tools it does not create one automatically.

\subsubsection{Evaluating Results}
The main challenge when testing file carving tools is verifying the results. 
For metadata-based tools this is fairly simple; the deleted files in our tests are all simple text files which can be checked at a glance.
However, text files lack a standard header and footer and thus cannot be recovered with file carving, so the test images use more complicated formats such as JPG.
As a result, it would be impractical to evaluate a tool based on the carved files alone.

Fortunately, all four of the carving tools we evaluated produce an itemized report of carving results. CFTT provides detailed information about the contents of the test images, so we can compare the tools' reports with the true arrangement of files in each image. This is particularly important for core feature 1, core feature 2, and core feature 3 since they are concerned with the specific data blocks the tool carves. PhotoRec's report gives a start and end address for each file, and multiple start and end addresses if it attempts to carve a fragmented file. The other tools' reports only list the starting address and size of each file, so we assumed they never attempt to carve more than one fragment per file.

Checking core feature 4 is slightly complicated by the fact that carving tools cannot recover the filename, but we can use the first sector listed for each file in the carving report to match them with the original files and check the file extensions.

For core feature 5, we make use of the \emph{identify} command from the ImageMagick~\cite{imagemagick} tool suite. Upon input of a graphical file, identify will output the file format and other information. Relevant to core feature 5 is that the return code will indicate whether the file is valid or corrupt. We add the \emph{-regard-warnings} flag so an error or warning while parsing the file will mean the file is considerd corrupt.

\subsubsection{Results}

\begin{figure}
    \centering

    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{fig/photorec_results_carve.pdf}
        \subcaption{PhotoRec}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{fig/foremost_results_carve.pdf}
        \subcaption{Foremost}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{fig/scalpel_results_carve.pdf}
        \subcaption{Scalpel}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{fig/axiom_results_carve.pdf}
        \subcaption{Magnet AXIOM}
    \end{subfigure}
        
    \caption{\TODO{working version of carving results}}
    \label{fig:results_carve}
\end{figure}

\TODO{TODO} 
